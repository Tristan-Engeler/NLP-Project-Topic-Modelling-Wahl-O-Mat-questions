{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6d0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Anna\n",
      "[nltk_data]     Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anna\n",
      "[nltk_data]     Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "<ipython-input-3-ac4baf8445fa>:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question']=df_text['Question'].str.lower()\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anna\n",
      "[nltk_data]     Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "<ipython-input-3-ac4baf8445fa>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question'] = df_text['Question'].apply(lambda X: word_tokenize(X))\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anna\n",
      "[nltk_data]     Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "<ipython-input-3-ac4baf8445fa>:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question'] = df_text['Question'].apply(remove_punct)\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anna\n",
      "[nltk_data]     Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "<ipython-input-3-ac4baf8445fa>:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question'] = df_text['Question'].apply(remove_stopwords)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anna Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Year                                           Question  \\\n",
      "0    2021  Auf allen Autobahnen soll ein generelles Tempo...   \n",
      "1    2021  Deutschland soll seine Verteidigungsausgaben e...   \n",
      "2    2021  Bei Bundestagswahlen sollen auch Jugendliche a...   \n",
      "3    2021  Die Förderung von Windenergie soll beendet wer...   \n",
      "4    2021  Die Möglichkeiten der Vermieterinnen und Vermi...   \n",
      "..    ...                                                ...   \n",
      "177  2005         Gesetzliche Gleichstellung der \"Homo-Ehe\"!   \n",
      "178  2005         Bei Bundestagswahlen: Wählen ab 16 Jahren!   \n",
      "179  2005  Biometrische Daten (z.B. Fingerabdruck) sollen...   \n",
      "180  2005                 Haschisch soll legalisiert werden.   \n",
      "181  2005              Volksentscheide auch auf Bundesebene!   \n",
      "\n",
      "                                  Titel  \n",
      "0             Tempolimit auf Autobahnen  \n",
      "1    Erhöhung der Verteidigungsausgaben  \n",
      "2                          Wählen ab 16  \n",
      "3                           Windenergie  \n",
      "4         Begrenzung für Mieterhöhungen  \n",
      "..                                  ...  \n",
      "177                                 NaN  \n",
      "178                                 NaN  \n",
      "179                                 NaN  \n",
      "180                                 NaN  \n",
      "181                                 NaN  \n",
      "\n",
      "[182 rows x 3 columns]\n",
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-ac4baf8445fa>:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question']=df_text['Question'].apply(lemmatization)  ## to Do: Spalte Date appenden\n",
      "<ipython-input-3-ac4baf8445fa>:88: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question']=df_text['Question'].str.lower()\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anna\n",
      "[nltk_data]     Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "<ipython-input-3-ac4baf8445fa>:95: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question'] = df_text['Question'].apply(lambda X: word_tokenize(X))\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Anna\n",
      "[nltk_data]     Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "<ipython-input-3-ac4baf8445fa>:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question'] = df_text['Question'].apply(remove_punct)\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Anna\n",
      "[nltk_data]     Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "<ipython-input-3-ac4baf8445fa>:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question'] = df_text['Question'].apply(remove_stopwords)\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anna Engler\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aber', 'alle', 'allem', 'allen', 'aller', 'alles', 'als', 'also', 'am', 'an', 'ander', 'andere', 'anderem', 'anderen', 'anderer', 'anderes', 'anderm', 'andern', 'anderr', 'anders', 'auch', 'auf', 'aus', 'bei', 'bin', 'bis', 'bist', 'da', 'damit', 'dann', 'der', 'den', 'des', 'dem', 'die', 'das', 'dass', 'daß', 'derselbe', 'derselben', 'denselben', 'desselben', 'demselben', 'dieselbe', 'dieselben', 'dasselbe', 'dazu', 'dein', 'deine', 'deinem', 'deinen', 'deiner', 'deines', 'denn', 'derer', 'dessen', 'dich', 'dir', 'du', 'dies', 'diese', 'diesem', 'diesen', 'dieser', 'dieses', 'doch', 'dort', 'durch', 'ein', 'eine', 'einem', 'einen', 'einer', 'eines', 'einig', 'einige', 'einigem', 'einigen', 'einiger', 'einiges', 'einmal', 'er', 'ihn', 'ihm', 'es', 'etwas', 'euer', 'eure', 'eurem', 'euren', 'eurer', 'eures', 'für', 'gegen', 'gewesen', 'hab', 'habe', 'haben', 'hat', 'hatte', 'hatten', 'hier', 'hin', 'hinter', 'ich', 'mich', 'mir', 'ihr', 'ihre', 'ihrem', 'ihren', 'ihrer', 'ihres', 'euch', 'im', 'in', 'indem', 'ins', 'ist', 'jede', 'jedem', 'jeden', 'jeder', 'jedes', 'jene', 'jenem', 'jenen', 'jener', 'jenes', 'jetzt', 'kann', 'kein', 'keine', 'keinem', 'keinen', 'keiner', 'keines', 'können', 'könnte', 'machen', 'man', 'manche', 'manchem', 'manchen', 'mancher', 'manches', 'mein', 'meine', 'meinem', 'meinen', 'meiner', 'meines', 'mit', 'muss', 'musste', 'nach', 'nicht', 'nichts', 'noch', 'nun', 'nur', 'ob', 'oder', 'ohne', 'sehr', 'sein', 'seine', 'seinem', 'seinen', 'seiner', 'seines', 'selbst', 'sich', 'sie', 'ihnen', 'sind', 'so', 'solche', 'solchem', 'solchen', 'solcher', 'solches', 'soll', 'sollte', 'sondern', 'sonst', 'über', 'um', 'und', 'uns', 'unsere', 'unserem', 'unseren', 'unser', 'unseres', 'unter', 'viel', 'vom', 'von', 'vor', 'während', 'war', 'waren', 'warst', 'was', 'weg', 'weil', 'weiter', 'welche', 'welchem', 'welchen', 'welcher', 'welches', 'wenn', 'werde', 'werden', 'wie', 'wieder', 'will', 'wir', 'wird', 'wirst', 'wo', 'wollen', 'wollte', 'würde', 'würden', 'zu', 'zum', 'zur', 'zwar', 'zwischen']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-ac4baf8445fa>:150: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_text['Question']=df_text['Question'].apply(lemmatization)  ## to Do: Spalte Date appenden\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[autobahn, generell, tempolimit, gelt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[deutschland, verteidigungsausgab, erhoh]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bundestagswahl, soll, jugend, ab, 16, jahr, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[forder, windenergi, beendet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[moglich, vermieterinn, vermiet, wohnungsmiet,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>[gesetz, gleichstell, homo, ehe]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>[bundestagswahl, wahl, ab, 16, jahr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>[biometr, dat, z, b, fingerabdruck, soll, pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>[haschisch, legalisiert]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>[volksentscheid, bundeseb]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>182 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Question\n",
       "0               [autobahn, generell, tempolimit, gelt]\n",
       "1            [deutschland, verteidigungsausgab, erhoh]\n",
       "2    [bundestagswahl, soll, jugend, ab, 16, jahr, w...\n",
       "3                        [forder, windenergi, beendet]\n",
       "4    [moglich, vermieterinn, vermiet, wohnungsmiet,...\n",
       "..                                                 ...\n",
       "177                   [gesetz, gleichstell, homo, ehe]\n",
       "178               [bundestagswahl, wahl, ab, 16, jahr]\n",
       "179  [biometr, dat, z, b, fingerabdruck, soll, pers...\n",
       "180                           [haschisch, legalisiert]\n",
       "181                         [volksentscheid, bundeseb]\n",
       "\n",
       "[182 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download('punkt') \n",
    "\n",
    "#import data\n",
    "#import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download('punkt') \n",
    "\n",
    "#import data\n",
    "df = pd.read_excel (r'C:\\Users\\Anna Engler\\OneDrive\\Dokumente/Wahl-O-Mat Questions.xlsx')\n",
    "print (df)\n",
    "\n",
    "#lower case\n",
    "df_text=df[['Question']]\n",
    "df_text['Question']=df_text['Question'].str.lower()\n",
    "df_text ## dump to csv -> write dataframe to csv -> df_text new dann neu hochladen  -> zwei scripte schreiben\n",
    "\n",
    "\n",
    "#tokenization\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "df_text['Question'] = df_text['Question'].apply(lambda X: word_tokenize(X))\n",
    "df_text.head()\n",
    "\n",
    "#remove punctuations/special characters - also numbers!!\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_punct(text):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    lst = tokenizer.tokenize(' '.join(text))\n",
    "    return lst\n",
    "\n",
    "df_text['Question'] = df_text['Question'].apply(remove_punct)\n",
    "df_text\n",
    "\n",
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('german'))\n",
    "\n",
    "de_stopwords = stopwords.words('german')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    result = []\n",
    "    for token in text:\n",
    "        if token not in de_stopwords:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result\n",
    "\n",
    "df_text['Question'] = df_text['Question'].apply(remove_stopwords)\n",
    "df_text\n",
    "\n",
    "#lemmatization\n",
    "\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "stemmer = GermanStemmer()\n",
    "from nltk import word_tokenize,pos_tag\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "\n",
    "def lemmatization(text):\n",
    "    \n",
    "    result=[]\n",
    "    wordnet = GermanStemmer()\n",
    "    for token,tag in pos_tag(text):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.stem(token)) #wir haben pos gelöscht, weil das sonst nicht funktioniert (3 statt 2 erlaubten Argumenten)\n",
    "    \n",
    "    return result\n",
    "\n",
    "df_text['Question']=df_text['Question'].apply(lemmatization)  ## to Do: Spalte Date appenden\n",
    "\n",
    "df_text\n",
    "\n",
    "#lower case\n",
    "df_text=df[['Question']]\n",
    "df_text['Question']=df_text['Question'].str.lower()\n",
    "df_text ## dump to csv -> write dataframe to csv -> df_text new dann neu hochladen  -> zwei scripte schreiben\n",
    "\n",
    "\n",
    "#tokenization\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "df_text['Question'] = df_text['Question'].apply(lambda X: word_tokenize(X))\n",
    "df_text.head()\n",
    "\n",
    "#remove punctuations/special characters - also numbers!!\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_punct(text):\n",
    "    \n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    lst = tokenizer.tokenize(' '.join(text))\n",
    "    return lst\n",
    "\n",
    "df_text['Question'] = df_text['Question'].apply(remove_punct)\n",
    "df_text\n",
    "\n",
    "#remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('german'))\n",
    "\n",
    "de_stopwords = stopwords.words('german')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    result = []\n",
    "    for token in text:\n",
    "        if token not in de_stopwords:\n",
    "            result.append(token)\n",
    "            \n",
    "    return result\n",
    "\n",
    "df_text['Question'] = df_text['Question'].apply(remove_stopwords)\n",
    "df_text\n",
    "\n",
    "#lemmatization\n",
    "\n",
    "from nltk.stem.snowball import GermanStemmer\n",
    "stemmer = GermanStemmer()\n",
    "from nltk import word_tokenize,pos_tag\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "\n",
    "def lemmatization(text):\n",
    "    \n",
    "    result=[]\n",
    "    wordnet = GermanStemmer()\n",
    "    for token,tag in pos_tag(text):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.stem(token)) #wir haben pos gelöscht, weil das sonst nicht funktioniert (3 statt 2 erlaubten Argumenten)\n",
    "    \n",
    "    return result\n",
    "\n",
    "df_text['Question']=df_text['Question'].apply(lemmatization)  ## to Do: Spalte Date appenden\n",
    "df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd2b26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Vectorization -> Aus Strings Floats machen\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# the vectorizer object will be used to transform text to vector form\n",
    "vectorizer = CountVectorizer(max_df=1, min_df=0)\n",
    "\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(df['Question']).toarray()\n",
    "\n",
    "# tf_feature_names tells us what word each column in the matric represents\n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169fc12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##topic modeling mit LDA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "number_of_topics = 20\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)\n",
    "\n",
    "model.fit(tf)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(model, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07de363",
   "metadata": {},
   "outputs": [],
   "source": [
    "## topic modeling mit NMF\n",
    "from sklearn.decomposition import NMF  ### schauen welche version which sklearn / sklearn__version - alpha_w \n",
    "\n",
    "number_of_topics = 15\n",
    "\n",
    "model = NMF(n_components=number_of_topics, random_state=None, alpha_H=0, l1_ratio=1)  ### alpha auf 0 setzen - was bedeuten andere parameters\n",
    "model.fit(tf)\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):    ## nur diese funktion über\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)\n",
    "\n",
    "no_top_words = 10\n",
    "display_topics(model, tf_feature_names, no_top_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
